{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "from sqlalchemy.sql import func as F\n",
    "from sqlalchemy.engine import create_engine, Engine\n",
    "from sqlalchemy.schema import Table, MetaData\n",
    "from sqlalchemy import Column, Integer, select, String, literal_column, text\n",
    "from sqlalchemy.sql.expression import Select, CompoundSelect, union_all\n",
    "from sqlalchemy_bigquery import ARRAY, STRUCT, INTEGER, FLOAT, FLOAT64 as DOUBLE, STRING\n",
    "from enum import Enum\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch GCP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = os.environ[\"GCP_PROJECT_ID\"]\n",
    "client = bigquery.Client(project=project)\n",
    "engine = create_engine(f\"bigquery://{project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Site:\n",
    "    ga4_property_id: str\n",
    "    \n",
    "DALLAS_FREE_PRESS = Site(ga4_property_id=os.environ[\"DALLAS_FREE_PRESS_GA4_PROPERTY_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_query_dates(end_date: date, num_days: int) -> List[date]:\n",
    "    return [end_date - timedelta(days=i) for i in range(0, num_days)]\n",
    "\n",
    "def construct_event_table_name(gcp_project_id: str, site: Site, dt: date) -> str:\n",
    "    return f\"{gcp_project_id}.analytics_{site.ga4_property_id}.events_{dt.strftime('%Y%m%d')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = 5\n",
    "query_dates = enumerate_query_dates(date(2023, 10, 15), num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrEnum(str, Enum):\n",
    "    def __repr__(self):\n",
    "        return self.value\n",
    "\n",
    "class ColumnBigQuery(StrEnum):\n",
    "    EVENT_TIMESTAMP = \"event_timestamp\"\n",
    "    EVENT_NAME = \"event_name\"\n",
    "    EVENT_PARAMS = \"event_params\"\n",
    "    USER_PSEUDO_ID = \"user_pseudo_id\"\n",
    "\n",
    "class ColumnNew(StrEnum):\n",
    "    EVENT_PAGE_LOCATION = \"event_page_location\"\n",
    "    EVENT_ENGAGEMENT_TIME_MSEC = \"event_engagement_time_msec\"\n",
    "\n",
    "def construct_query_single_table(table_name: str, engine: Engine) -> Select:\n",
    "    table = Table(\n",
    "        table_name, \n",
    "        MetaData(bind=engine), \n",
    "        Column(ColumnBigQuery.EVENT_TIMESTAMP, Integer),\n",
    "        Column(ColumnBigQuery.EVENT_NAME, String),\n",
    "        Column(ColumnBigQuery.EVENT_PARAMS, ARRAY(STRUCT(key=STRING, value=STRUCT(string_value=STRING, int_value=INTEGER, float_value=DOUBLE)))),\n",
    "        Column(ColumnBigQuery.USER_PSEUDO_ID, String),\n",
    "        autoload_with=engine\n",
    "    )\n",
    "\n",
    "    subquery_page_location = select(\n",
    "        literal_column(\"value.string_value\")\n",
    "    ).select_from(\n",
    "        F.unnest(table.c.event_params).alias(\"params\")\n",
    "    ).where(\n",
    "        literal_column(\"params.key\") == \"page_location\"\n",
    "    ).label(ColumnNew.EVENT_PAGE_LOCATION)\n",
    "\n",
    "    subquery_engagement_time = select(\n",
    "        literal_column(\"value.int_value\")\n",
    "    ).select_from(\n",
    "        F.unnest(table.c.event_params).alias(\"params\")\n",
    "    ).where(\n",
    "        literal_column(\"params.key\") == \"engagement_time_msec\"\n",
    "    ).label(ColumnNew.EVENT_ENGAGEMENT_TIME_MSEC)\n",
    "\n",
    "    query = select(\n",
    "        table.c.event_timestamp,\n",
    "        table.c.event_name,\n",
    "        subquery_page_location,\n",
    "        subquery_engagement_time,\n",
    "        table.c.user_pseudo_id\n",
    "    ).order_by(\n",
    "        table.c.user_pseudo_id,\n",
    "        table.c.event_timestamp\n",
    "    )\n",
    "\n",
    "    return query\n",
    "\n",
    "def construct_query(query_dates: List[date], site: Site, engine: Engine) -> Select:\n",
    "    queries = [construct_query_single_table(construct_event_table_name(project, site, dt), engine) for dt in query_dates]\n",
    "    subquery = union_all(*queries).subquery()\n",
    "    return select(subquery) # Because SQLAlchemy is dumb and doesn't allow direct stringification of a CompoundSelect\n",
    "\n",
    "def stringify_query(query: Select, engine: Engine) -> str:\n",
    "    return str(query.compile(engine, compile_kwargs={\"literal_binds\": True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = construct_query(query_dates, site=DALLAS_FREE_PRESS, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = stringify_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Even if a query is less than 10 MiB, BigQuery still charges as if the query is 10 MiB. For queries less than 10 MiB, better to batch them together so the resulting query is at least 10 MiB, cutting cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_table_query_tib_processed(query_str: str, client: bigquery.Client) -> float:\n",
    "    \"\"\"\n",
    "    Return number of bytes processed by a query\n",
    "    \"\"\"\n",
    "    return client.query(query_str, job_config=bigquery.QueryJobConfig(dry_run=True)).total_bytes_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_processed = calculate_table_query_tib_processed(query_str, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiBs processed\n",
    "f\"{bytes_processed / 1024**2:.2f} MiB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TiBs processed\n",
    "f\"{bytes_processed / 1024**4} TiB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082787"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_table(construct_event_table_name(project, DALLAS_FREE_PRESS, dt=query_dates[0])).num_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to make sure we're not wasting money given the 10 MiB minimum charge per query\n",
    " TODO: A function that, given a site name, a list of dates, and a query bytes floor of FLOOR = 10 MiB & some CEIL,\n",
    "- first, iterate along the list of dates until the cumulative total storage of the tables is greater than FLOOR\n",
    "- then, interate and build query string as we go, until total bytes processed is just under CEIL\n",
    "- return the query string\n",
    "- if there are more dates left, repeat the process\n",
    "\n",
    "\n",
    "Or caching: https://cloud.google.com/bigquery/docs/cached-results, but caches are only valid for at most 24 hours.\n",
    "Could instead build our own cache; construct a dedicated events table that only keeps fetched events up to a certain threshold (say, 1 month if 1 month of events are needed to train recs; anything older would be deleted), \n",
    "but for small sites (and even bigger ones) this is probably overkill, though would still be nice to have "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
